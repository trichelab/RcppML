[{"path":"/articles/annotating_nmf_models.html","id":"annotating-nmf-factors","dir":"Articles","previous_headings":"","what":"Annotating NMF factors","title":"Learning and Annotating NMF Models","text":"NMF learns interpretable low-rank representation data. However, make sense factors low-rank latent model? great way begin annotating latent space simply map back known sample feature traits. vignette demonstrates concepts using NMF model bird species communities throughout Hawaiian islands.","code":""},{"path":"/articles/annotating_nmf_models.html","id":"install-rcppml","dir":"Articles","previous_headings":"","what":"Install RcppML","title":"Learning and Annotating NMF Models","text":"Install RcppML R package CRAN development version GitHub.","code":"install.packages('RcppML')                     # install CRAN version # devtools::install_github(\"zdebruine/RcppML\") # compile dev version library(RcppML) library(ggplot2) library(cowplot) library(viridis) library(ggrepel) library(uwot)"},{"path":"/articles/annotating_nmf_models.html","id":"the-hawaiibirds-dataset","dir":"Articles","previous_headings":"","what":"The hawaiibirds dataset","title":"Learning and Annotating NMF Models","text":"hawaiibirds dataset gives frequency bird species small geographical grids throughout state Hawaii. separate metadata_h matrix gives geographical coordinates corresponding island grid. separate metadata_w matrix gives taxonomic information species database.","code":"data(hawaiibirds) hawaiibirds$counts[1:4, 1:4] ## 4 x 4 sparse Matrix of class \"dgCMatrix\" ##                                grid1      grid2       grid3      grid4 ## Common Myna               0.32432432 0.19230769 0.242753623 0.80208333 ## Black-crowned Night-Heron 0.06756757 0.07692308 0.007246377 0.03819444 ## Black Noddy               .          0.26923077 0.188405797 .          ## Brown Noddy               .          0.38461538 .           . head(hawaiibirds$metadata_h) ##    grid island   lat     lng ## 1 grid1   Maui 20.87 -156.44 ## 2 grid2   Oahu 21.33 -157.66 ## 3 grid3 Hawaii 19.33 -155.19 ## 4 grid4   Oahu 21.37 -157.94 ## 5 grid5 Hawaii 19.72 -155.11 ## 6 grid6   Maui 20.74 -156.24 head(hawaiibirds$metadata_w) ##                     species     status           type ## 1               Common Myna introduced perching birds ## 2 Black-crowned Night-Heron     native         waders ## 3               Black Noddy     native     shorebirds ## 4               Brown Noddy     native     shorebirds ## 5           Bulwer's Petrel     native       seabirds ## 6                Sooty Tern     native     shorebirds"},{"path":"/articles/annotating_nmf_models.html","id":"cross-validation-for-rank-determination","dir":"Articles","previous_headings":"","what":"Cross-validation for Rank Determination","title":"Learning and Annotating NMF Models","text":"can learn NMF model describe linear combinations species across geographical grids. First need choose rank. rank factorization crucial hyperparameter. One way help decide rank cross-validation. made easy using crossValidate function. See ?crossValidate details methods. many applications, “optimal” rank. case, expect amount distinct biodiversity across various islands, within islands continuum habitat niches confounding rank signal. Additionally, may number “missing” observations surveys incomplete, confound signal separation. cross-validate across 3 independent replicates plot result (code evaluated vignette since takes minute execute): ’ll choose rank k = 15 since seems return best prediction accuracy.","code":"plot(crossValidate(hawaiibirds$counts, k = c(1:20), reps = 3)) + scale_y_continuous(trans = \"log10\")"},{"path":"/articles/annotating_nmf_models.html","id":"run-robust-nmf","dir":"Articles","previous_headings":"","what":"Run robust NMF","title":"Learning and Annotating NMF Models","text":"Let’s generate high-quality NMF model across 3 random restarts low tolerance: w matrix factors describing communities co-occuring bird species. h matrix association bird communities surveyed geographical grid.","code":"model <- nmf(hawaiibirds$counts, k = 15, seed = 1:3, tol = 1e-6) model ## 183 x 1183 x 15 factor model of class \"nmf\" ## @ w ##                                  nmf1        nmf2        nmf3        nmf4 ## Common Myna               0.148328389 0.097004277 0.070322627 0.126175888 ## Black-crowned Night-Heron 0.007912592 0.009766297           . 0.006037977 ## Black Noddy                         .           .           . 0.009565077 ## Brown Noddy                         .           .           .           . ## Bulwer's Petrel                     .           .           .           . ## Sooty Tern                          .           . 0.001327280           . ## Wedge-tailed Shearwater             .           .           .           . ##                                  nmf5        nmf6 nmf7 ## Common Myna               0.024307396 0.001601450    . ## Black-crowned Night-Heron 0.087244449           .    . ## Black Noddy                         .           .    . ## Brown Noddy                         .           .    . ## Bulwer's Petrel                     .           .    . ## Sooty Tern                          .           .    . ## Wedge-tailed Shearwater             .           .    . ## ...suppressing 176 rows and 8 columns ##  ## @ d ## 1180.546  991.8922  869.0251  833.8436  686.2124  684.9236  559.2632   ## ...suppressing 8 values ##  ## @ h ##             grid1        grid2        grid3        grid4        grid5 ## nmf1 8.912460e-04 6.376840e-04 5.574931e-04 3.221175e-03 1.039969e-03 ## nmf2 7.929197e-04            . 1.946878e-04            . 3.430622e-05 ## nmf3 2.719171e-04 4.056909e-04            . 4.664333e-03            . ## nmf4            .            . 1.187724e-03 1.497042e-04 1.682771e-03 ## nmf5 5.350900e-03 4.269131e-05            . 3.384526e-04            . ## nmf6 4.803149e-04 4.019916e-04 4.962776e-04 2.990747e-05 1.287416e-03 ## nmf7            .            . 1.156676e-03            .            . ##             grid6        grid7 ## nmf1            . 5.320816e-04 ## nmf2 2.434128e-05 3.503179e-03 ## nmf3            . 3.180302e-04 ## nmf4            . 7.406379e-05 ## nmf5            .            . ## nmf6            . 9.921460e-05 ## nmf7            .            . ## ...suppressing 8 rows and 1176 columns ##  ## @ misc ## List of 5 ##  $ tol    : num 9.7e-07 ##  $ iter   : num 51 ##  $ runtime: 'difftime' num 0.452081918716431 ##   ..- attr(*, \"units\")= chr \"secs\" ##  $ mse    : num 0.00274 ##  $ w_init : num [1:15, 1:183] 1.038 1.707 2.259 0.848 2.196 ..."},{"path":"/articles/annotating_nmf_models.html","id":"geographic-focus-on-nmf-factors","dir":"Articles","previous_headings":"","what":"Geographic focus on NMF factors","title":"Learning and Annotating NMF Models","text":"NMF factor tell us? sample embeddings matrix (h) gives information geographical representation NMF factor across grids. ’ll look just first four factors:","code":"plots <- list() for(i in 1:4){   df <- data.frame(     \"lat\" = hawaiibirds$metadata_h$lat,     \"lng\" = hawaiibirds$metadata_h$lng,     \"nmf_factor\" = model$h[i, ])   plots[[i]] <- ggplot(df, aes(x = lng, y = lat, color = nmf_factor)) +     geom_point() +     scale_color_viridis(option = \"B\") +     theme_void() +     theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5)) +      ggtitle(paste0(\"Factor \", i)) } plot_grid(plotlist = plots, nrow = 2)"},{"path":"/articles/annotating_nmf_models.html","id":"metadata-enrichment-in-factors","dir":"Articles","previous_headings":"","what":"Metadata enrichment in factors","title":"Learning and Annotating NMF Models","text":"Factors can capture island-restricted bird species information also information shared across islands. Quantitatively, summary method nmf S4 class makes easy annotate factors using metadata samples features. case, use summary map factor enrichment grids corresponding Hawaiian island, species enrichment corresponding type species.  general, grids separate based island – consistent expectation islands contain distinct species communities.  Notice type bird species tend co-occur – waders perching birds waterfowl, waterfowl waders, seabirds shorebirds, many different perching bird contexts.","code":"plot(summary(model, group_by = hawaiibirds$metadata_h$island, stat = \"mean\")) plot(summary(model, group_by = hawaiibirds$metadata_w$type, stat = \"mean\"))"},{"path":"/articles/annotating_nmf_models.html","id":"nmf-biplots","dir":"Articles","previous_headings":"","what":"NMF biplots","title":"Learning and Annotating NMF Models","text":"Compare species composition two factors, instance factor 2 3:","code":"biplot(model, factors = c(2, 3), matrix = \"w\", group_by = hawaiibirds$metadata_w$type) +    scale_y_continuous(trans = \"sqrt\") +    scale_x_continuous(trans = \"sqrt\") +   geom_text_repel(size = 2.5, seed = 123, max.overlaps = 15) +   theme(aspect.ratio = 1)"},{"path":"/articles/annotating_nmf_models.html","id":"umap-on-nmf-embeddings","dir":"Articles","previous_headings":"","what":"UMAP on NMF embeddings","title":"Learning and Annotating NMF Models","text":"might also interested visualizing factors w capture similarities among bird species using UMAP.  Species classified based habitat niche taxonomic membership. also two groups “waterfowl”, consistent ocean shoreline inland wetland niches. Hawaii bird species extinction kingdom: 20 species endemic honeycreeper gone extinct past two centuries due establishment introduced species habitat devastation. remain. UMAP plot right, can observe introduced species dominate habitat niches occupied native perching non-perching birds, problem underlying historic ongoing mass extinction events.  Islands also well-defined NMF model.","code":"set.seed(123) umap <- data.frame(uwot::umap(model$w)) umap$taxon <- hawaiibirds$metadata_w$type umap$status <- hawaiibirds$metadata_w$status plot_grid(   ggplot(umap, aes(x = umap[,1], y = umap[,2], color = taxon)) +     geom_point() + theme_void(),   ggplot(umap, aes(x = umap[,1], y = umap[,2], color = status)) +     geom_point() + theme_void(),   nrow = 1 ) set.seed(123) umap <- data.frame(uwot::umap(t(model$h), metric = \"cosine\")) umap$group <- hawaiibirds$metadata_h$island ggplot(umap, aes(x = umap[,1], y = umap[,2], color = group)) +   geom_point() + theme_void() + theme(aspect.ratio = 1)"},{"path":"/articles/annotating_nmf_models.html","id":"defining-the-palila-species-niche","dir":"Articles","previous_headings":"","what":"Defining the “Palila” species niche","title":"Learning and Annotating NMF Models","text":"Palila highly endangered species survives small numbers southwestern slopes Mauna Kea shrubby dry “rainforest” biome, characterized last stands endemic Mamame trees. species coexist Palila? Let’s look species composition factor highest Palila representation, specifically identifying species introduced native:  diet Palilla largely seeds “mamame” tree, also naio berries mamame flowers, buds, young leaves. introduced perching birds may competing Palila resources? “House Finch” “Yellow-fronted Canary” seem significant competitors Palila habitat niche.","code":"ggplot(data.frame(\"value\" = model$w[\"Palila\", ], \"factor\" = 1:ncol(model$w)), aes(factor, value)) +    geom_point() +    theme_classic() +    theme(aspect.ratio = 1) +    labs(x = \"NMF factor\", y = \"Palila weight in NMF factor\") df <- data.frame(\"value\" = model$w[, which.max(model$w[\"Palila\", ])]) df$status <- hawaiibirds$metadata_w$status df <- df[order(-df$value), ] df <- df[df$value > 0.001, ] df ##                             value     status ## Hawaii Amakihi        0.429048285     native ## Warbling White-eye    0.197932914     native ## House Finch           0.165412451 introduced ## California Quail      0.036549864     native ## Yellow-fronted Canary 0.035886958 introduced ## Palila                0.033441821     native ## Erckel's Francolin    0.028694271 introduced ## Hawaii Elepaio        0.025463511     native ## Eurasian Skylark      0.025317139 introduced ## Red-billed Leiothrix  0.006118122 introduced ## Chukar                0.006071825 introduced ## Chinese Hwamei        0.005362257 introduced ## Hawaiian Hawk         0.001774794     native ## Red-masked Parakeet   0.001499308 introduced perching_birds <- hawaiibirds$metadata_w$species[hawaiibirds$metadata_w$type == \"perching birds\"] df[which(rownames(df) %in% perching_birds & df$status == \"introduced\"), ] ##                             value     status ## House Finch           0.165412451 introduced ## Yellow-fronted Canary 0.035886958 introduced ## Eurasian Skylark      0.025317139 introduced ## Red-billed Leiothrix  0.006118122 introduced ## Chinese Hwamei        0.005362257 introduced"},{"path":"/articles/getting_started.html","id":"install-rcppml","dir":"Articles","previous_headings":"","what":"Install RcppML","title":"Getting Started with NMF","text":"Install RcppML R package CRAN development version GitHub.","code":"install.packages('RcppML')                     # install CRAN version # devtools::install_github(\"zdebruine/RcppML\") # compile dev version"},{"path":"/articles/getting_started.html","id":"what-is-nmf","dir":"Articles","previous_headings":"","what":"What is NMF?","title":"Getting Started with NMF","text":"Non-negative Matrix Factorization (NMF) finds additive signals non-negative data terms features samples associated signals. NMF gives approximation input matrix cross-product two low-rank submatrices: \\[= wdh\\] , \\(\\) input matrix, \\(w\\) tall matrix features rows factors columns, \\(h\\) wide matrix factors rows samples columns. RcppML::nmf introduces one important component system, scaling diagonal, \\(d\\). scaling diagonal provides: consistent factor scalings throughout model fitting robustness across random restarts symmetry factorization symmetric matrices means convex L1 regularization","code":""},{"path":"/articles/getting_started.html","id":"running-nmf","dir":"Articles","previous_headings":"","what":"Running NMF","title":"Getting Started with NMF","text":"Run NMF iris dataset. need specify rank (k) factorization, also specify seed random initialization reproducibility:","code":"library(RcppML) library(Matrix) library(ggplot2) library(cowplot)  data(iris) model <- nmf(iris[,1:4], k = 3, seed = 1) model #> 150 x 4 x 3 factor model of class \"nmf\" #> @ w #>              nmf1         nmf2         nmf3 #> [1,] 0.0014403134 0.0002357974 0.0147753897 #> [2,] 0.0024259960            . 0.0129176731 #> [3,] 0.0014543773 0.0001794741 0.0134988417 #> [4,] 0.0019489186 0.0005632185 0.0124668937 #> [5,] 0.0001151302 0.0015972125 0.0148072020 #> [6,]            . 0.0030264275 0.0151420941 #> [7,]            . 0.0022133280 0.0132919365 #> ...suppressing 143 rows #>  #> @ d #> 747.8924  726.0631  605.7889   #>  #> @ h #>      Sepal.Length Sepal.Width Petal.Length Petal.Width #> nmf1  0.426857790 0.123598613  0.355488027 0.094055570 #> nmf2  0.340537544 0.193888276  0.320817305 0.144756875 #> nmf3  0.511768434 0.372576543  0.107677454 0.007977569 #>  #> @ misc #> List of 4 #>  $ tol    : num 9.76e-05 #>  $ iter   : num 14 #>  $ runtime: 'difftime' num 0.0313060283660889 #>   ..- attr(*, \"units\")= chr \"secs\" #>  $ w_init : num [1:3, 1:150] 0.266 0.372 0.573 0.908 0.202 ..."},{"path":"/articles/getting_started.html","id":"visualizing-nmf-models","dir":"Articles","previous_headings":"","what":"Visualizing NMF Models","title":"Getting Started with NMF","text":"result RcppML::nmf S34object class nmf. nmf class many useful methods: One useful methods summary (turn plot method):  Notice NMF factors capture variable information among iris species. biplot method NMF (see ?biplot.nmf details) can compare weights different features samples two factors:","code":"methods(class = \"nmf\") #>  [1] $        [        [[       align    biplot   coerce   dim      dimnames #>  [9] evaluate head     predict  prod     show     sort     sparsity subset   #> [17] summary  t        #> see '?methods' for accessing help and source code species_stats <- summary(model, group_by = iris$Species) species_stats #>        group factor       stat #> 1     setosa   nmf1 0.06364741 #> 2 versicolor   nmf1 0.46947182 #> 3  virginica   nmf1 0.46688076 #> 4     setosa   nmf2 0.04650387 #> 5 versicolor   nmf2 0.31381639 #> 6  virginica   nmf2 0.63967974 #> 7     setosa   nmf3 0.70431244 #> 8 versicolor   nmf3 0.22449607 #> 9  virginica   nmf3 0.07119149 plot(species_stats, stat = \"sum\") biplot(model, factors = c(1, 2), group_by = iris$Species)"},{"path":"/articles/getting_started.html","id":"random-restarts","dir":"Articles","previous_headings":"","what":"Random Restarts","title":"Getting Started with NMF","text":"NMF randomly initialized, thus results may slightly different every time. run NMF many times, set multiple seeds, best model returned. run 10 factorizations higher tolerance, best model returned: second model better.","code":"model2 <- nmf(iris[,1:4], k = 3, seed = 1:10, tol = 1e-5) # MSE of model from single random initialization evaluate(model, iris[,1:4]) #> [1] 0.007247329  # MSE of best model among 10 random restarts evaluate(model2, iris[,1:4]) #> [1] 0.006100573"},{"path":"/articles/getting_started.html","id":"l1-regularization","dir":"Articles","previous_headings":"","what":"L1 Regularization","title":"Getting Started with NMF","text":"Sparse factors contain non-zero values make easy identify features samples important. L1/LASSO regularization best method introducing sparsity linear model. sparsity S3 method class nmf makes easy compute sparsity factors, done . Note mask_zeros = TRUE example . zero-valued ratings missing, thus considered factorization. example, regularized \\(w\\) \\(h\\), however model can also regularized separately:  Note side model regularized independently. L1 regularization significantly affect model loss: L1 regularization also significantly affect model information low penalties. measure cost bipartite matching two models cosine distance matrix L1 = 0, L1 = 0.01, L1 = 0.1: cosine angles (range 0 1) small – words, models similar. See ?RcppML::cosine details fast computation cosine similarity. code, computed cosine distance subtracting cosine similarity 1, matched cost matrix, divided 10 find mean cosine distance matched factors. cases, factors correspond well. Thus, regularized RcppML::nmf increases factor sparsity without significantly affecting loss information content model.","code":"data(movielens) ratings <- movielens$ratings model_L1 <- nmf(ratings, k = 7, L1 = 0.1, seed = 123, mask_zeros = TRUE) sparsity(model_L1) #>    factor  sparsity model #> 1    nmf1 0.6193432     w #> 2    nmf2 0.8774244     w #> 3    nmf3 0.6346005     w #> 4    nmf4 0.6030515     w #> 5    nmf5 0.8986294     w #> 6    nmf6 0.2927334     w #> 7    nmf7 0.4569434     w #> 8    nmf1 0.6786885     h #> 9    nmf2 0.2819672     h #> 10   nmf3 0.6540984     h #> 11   nmf4 0.7114754     h #> 12   nmf5 0.4311475     h #> 13   nmf6 0.9245902     h #> 14   nmf7 0.9426230     h model_no_L1 <- nmf(ratings, k = 7, L1 = 0, seed = 123, mask_zeros = TRUE) model_L1_h <-  nmf(ratings, k = 7, L1 = c(0, 0.1), seed = 123, mask_zeros = TRUE) model_L1_w <-  nmf(ratings, k = 7, L1 = c(0.1, 0), seed = 123, mask_zeros = TRUE)  # summarize sparsity of all models in a data.frame df <- rbind(sparsity(model_no_L1), sparsity(model_L1_h), sparsity(model_L1_w), sparsity(model_L1)) df$side <- c(rep(\"none\", 14), rep(\"h only\", 14), rep(\"w only\", 14), rep(\"both\", 14)) df$side <- factor(df$side, levels = unique(df$side)) ggplot(df, aes(x = side, y = sparsity, color = model)) +    geom_boxplot(outlier.shape = NA, width = 0.6) +    geom_point(position = position_jitterdodge()) + theme_classic() +    labs(x = \"Regularized side of model\", y = \"sparsity of model factors\") # L1 = 0 evaluate(model_no_L1, movielens$ratings, mask = \"zeros\") #> [1] 6.911534  # L1 = 0.1 evaluate(model_L1, movielens$ratings, mask = \"zeros\") #> [1] 7.418375 model_low_L1 <- nmf(movielens$ratings, k = 5, L1 = 0.01, seed = 123) # cost of bipartite matching: L1 = 0 vs. L1 = 0.01 bipartiteMatch(1 - cosine(model_no_L1$w, model_low_L1$w))$cost / 10 #> [1] 0.03909451  # cost of bipartite matching: L1 = 0 vs. L1 = 0.1 bipartiteMatch(1 - cosine(model_no_L1$w, model_L1$w))$cost / 10 #> [1] 0.01713988"},{"path":"/articles/getting_started.html","id":"predictionrecommendation-with-nmf","dir":"Articles","previous_headings":"","what":"Prediction/Recommendation with NMF","title":"Getting Started with NMF","text":"NMF models learned samples can projected samples, common routine recommendation systems transfer learning. instance, may train model samples use predict values samples. instance, dataset predict bird species likely encountered grid land given information just fraction species.","code":"data(hawaiibirds) A <- hawaiibirds$counts  test_grids <- sample(1:ncol(A), ncol(A) / 5) test_species <- sample(1:nrow(A), nrow(A) * 0.5)  # construct a sparse masking matrix for these species and grids mask <- matrix(0, nrow(A), ncol(A)) mask[test_species, test_grids] <- 1 mask <- as(mask, \"dgCMatrix\")  model <- nmf(A, k = 15, mask = mask, tol = 1e-6, seed = 123)  df <- rbind(   data.frame(     \"observed\" = as.vector(A[test_species, test_grids]),     \"predicted\" = as.vector(prod(model)[test_species, test_grids]),     \"set\" = \"test\"   ), data.frame(     \"observed\" = as.vector(A[-test_species, -test_grids]),     \"predicted\" = as.vector(prod(model)[-test_species, -test_grids]),     \"set\" = \"train\"   ) )  ggplot(df, aes(observed, predicted, color = set)) +    theme_classic() +    theme(aspect.ratio = 1) +    scale_y_continuous(expand = c(0, 0), limits = c(0, 1), trans = \"sqrt\") +    scale_x_continuous(expand = c(0, 0), limits = c(0, 1), trans = \"sqrt\") +    geom_point(size = 0.5) +    facet_grid(cols = vars(set)) +    theme(legend.position = \"none\") #> Warning: Removed 150 rows containing missing values (geom_point)."},{"path":"/articles/getting_started.html","id":"cross-validation-for-rank-determination","dir":"Articles","previous_headings":"","what":"Cross-validation for rank determination","title":"Getting Started with NMF","text":"Cross-validation can assist finding reasonable factorization rank. determine optimal rank aml dataset using cross-validation across three random replicates: Use S4 plot method nmfCrossValidation class visualize:  optimal rank around k = 8.","code":"data(aml) cv_data <- crossValidate(aml$data, k = 1:10, reps = 3) head(cv_data) #>   rep k      value #> 1   1 1 0.03619901 #> 2   1 2 0.02974895 #> 3   1 3 0.02757322 #> 4   1 4 0.02599658 #> 5   1 5 0.02490966 #> 6   1 6 0.02413747 plot(cv_data)"},{"path":"/articles/robust_nmf.html","id":"reproducibility","dir":"Articles","previous_headings":"","what":"Reproducibility","title":"Robust NMF with random initializations","text":"Simply, set seed calling function.","code":"library(RcppML) #> RcppML v0.5.5 using 'options(RcppML.threads = 0)' (all available threads), 'options(RcppML.verbose = FALSE)' A <- r_sparsematrix(1000, 1000, inv_density = 16) not_reproducible_model <- nmf(A, k = 10, seed = NULL) # default reproducible_model <- nmf(A, k = 10, seed = 123)"},{"path":"/articles/robust_nmf.html","id":"random-initialization","dir":"Articles","previous_headings":"","what":"Random initialization","title":"Robust NMF with random initializations","text":"Random uniform initializations best initializations local minima assume prior distribution information. Using non-random initializations (like NNDSVD, first proposed Boutsidis) can trap models dangerous local minima mandate model inspired orthogonality rather colinearity.","code":""},{"path":"/articles/robust_nmf.html","id":"random-restarts-with-rcppml","dir":"Articles","previous_headings":"","what":"Random restarts with RcppML","title":"Robust NMF with random initializations","text":"Let’s see multiple restarts can improve model. RcppML, simply specify multiple seeds run multiple restarts: Multiple random restarts help discover better models. happens single seed specified? RcppML uses runif generate uniform distribution randomly selected range. rnorm used just safe, sometimes rnorm can actually give worse results. happens multiple seeds specified? RcppML generates multiple initializations, initialization randomly choosing use runif rnorm, randomly selecting uniform range mean standard deviation, respectively.","code":"data(hawaiibirds) m1 <- nmf(hawaiibirds$counts, k = 10, seed = 1) m2 <- nmf(hawaiibirds$counts, k = 10, seed = 1:10) evaluate(m1, hawaiibirds$counts) #> [1] 0.003552783 evaluate(m2, hawaiibirds$counts) #> [1] 0.003450242"},{"path":"/articles/robust_nmf.html","id":"refining-nmf-models-from-warm-starts","dir":"Articles","previous_headings":"","what":"Refining NMF models from warm starts","title":"Robust NMF with random initializations","text":"Suppose learn NMF model want come back later fit . Alternatively, might learn model one set samples, simply want fit another set samples. learn NMF model half survey grids hawaiibirds dataset, fit half: use used model1 “warm start” training model2, particularly helpful. Suppose trained model2 “cold start”, done better (measured MSE)? slightly better. conclusion, almost always better start random initialization. Using prior information rarely helpful appreciably speed fitting process.","code":"A <- hawaiibirds$counts grids1 <- sample(1:ncol(A), floor(ncol(A)/2)) grids2 <- (1:ncol(A))[-grids1]  model1 <- nmf(A[, grids1], k = 15, seed = 123) model2 <- nmf(A[, grids2], k = 15, seed = model1@w)  cat(\"model 1 iterations: \", model1@misc$iter,     \", model 2 iterations: \", model2@misc$iter) #> model 1 iterations:  15 , model 2 iterations:  33 model2_new <- nmf(A[, grids2], k = 15, seed = 123)  cat(\"model 2 warm-start\", evaluate(model2_new, A[, grids2]),     \", model 2 random start: \", evaluate(model2, A[, grids2])) #> model 2 warm-start 0.002776528 , model 2 random start:  0.002686309"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Zach DeBruine. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"DeBruine Z (2022). RcppML: Rcpp Machine Learning Library. R package version 0.5.5, https://github.com/zdebruine/RcppML.","code":"@Manual{,   title = {RcppML: Rcpp Machine Learning Library},   author = {Zach DeBruine},   year = {2022},   note = {R package version 0.5.5},   url = {https://github.com/zdebruine/RcppML}, }"},{"path":"/index.html","id":"rcpp-machine-learning-library","dir":"","previous_headings":"","what":"Rcpp Machine Learning Library","title":"Rcpp Machine Learning Library","text":"OOPS: Accidental commit, restore version 9/24/2022 9/26/2022 6pm EST.  RcppML R package fast non-negative matrix factorization divisive clustering using large sparse matrices. See pkgdown site : https://zdebruine.github.io/RcppML/ RcppML NMF : * fastest NMF implementation language sparse dense matrices * interpretable implementations due diagonal scaling * Easy regularize L1 penalty","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Rcpp Machine Learning Library","text":"Install CRAN development version GitHub: NOTE: RcppML actively developed. Please check packageVersion(\"RcppML\") current raising issues. Check CRAN manual. installed loaded, RcppML C++ headers defining classes can used C++ files R package using #include <RcppML.hpp>.","code":"install.packages('RcppML')                       # install CRAN version devtools::install_github(\"zdebruine/RcppML\")     # compile dev version"},{"path":"/index.html","id":"matrix-factorization","dir":"","previous_headings":"","what":"Matrix Factorization","title":"Rcpp Machine Learning Library","text":"Sparse matrix factorization alternating least squares: * Non-negativity constraints * L1 regularization * Diagonal scaling * Rank-1 Rank-2 specializations (~2x faster irlba SVD equivalents) Read (cite) bioRXiv manuscript NMF single-cell experiments.","code":""},{"path":"/index.html","id":"r-functions","dir":"","previous_headings":"Matrix Factorization","what":"R functions","title":"Rcpp Machine Learning Library","text":"nmf function runs matrix factorization alternating least squares form = WDH. project function updates w h given , mse function calculates mean squared error factor model.","code":"library(RcppML) A <- Matrix::rsparsematrix(1000, 100, 0.1) # sparse Matrix::dgCMatrix model <- RcppML::nmf(A, k = 10) h0 <- predict(model, A) evaluate(model, A) # calculate mean squared error"},{"path":"/index.html","id":"divisive-clustering","dir":"","previous_headings":"","what":"Divisive Clustering","title":"Rcpp Machine Learning Library","text":"Divisive clustering rank-2 spectral bipartitioning. * 2nd SVD vector linearly related difference factors rank-2 matrix factorization. * Rank-2 matrix factorization (optional non-negativity constraints) spectral bipartitioning ~2x faster irlba SVD * Sensitive distance-based stopping criteria similar Newman-Girvan modularity, orders magnitude faster * Stopping criteria based minimum number samples","code":""},{"path":"/index.html","id":"r-functions-1","dir":"","previous_headings":"Divisive Clustering","what":"R functions","title":"Rcpp Machine Learning Library","text":"dclust function runs divisive clustering recursive spectral bipartitioning, bipartition function exposes rank-2 NMF specialization returns statistics bipartition.","code":"library(RcppML) A <- Matrix::rsparsematrix(1000, 1000, 0.1) # sparse Matrix::dgcMatrix clusters <- dclust(A, min_dist = 0.001, min_samples = 5) cluster0 <- bipartition(A)"},{"path":[]},{"path":"/news.html","id":"major-changes","dir":"","previous_headings":"RcppML 0.5.0","what":"Major changes","title":"NA","text":"Launch pkgdown site Added nmf S3 class result nmf function Introduce S3 methods NMF ([, align, biplot, dim, dimnames, head, mse, predict, print, prod, sort, sparsity, summary, t) New plotting methods NMF (biplot.nmf, plot.nmfSummary, plot.nmfCrossValidation) mse now S3 method nmf objects project now handles projections w, simplicity New vignette Getting Started NMF!","code":""},{"path":"/news.html","id":"minor-changes","dir":"","previous_headings":"RcppML 0.5.0","what":"Minor changes","title":"NA","text":"Support specific sample feature selections NMF removed increase performance C++ end Removed updateInPlace advanced parameter nmf advantages convincing mask_zeros implementation now specific sparse matrices, multi-thread parallelization, projections transposition Added cosine function fast cosine distance calculations Condensed pared documentation throughout. Advanced usage discussion moved future vignettes.","code":""},{"path":[]},{"path":"/news.html","id":"major-changes-1","dir":"","previous_headings":"RcppML 0.5.1","what":"Major changes","title":"NA","text":"three new datasets (hawaiibirds, aml, movielens) Move NMF models methods S3 S4 stability Better random initializations (now using rnorm runif multiple ranges/shapes, multiple seeds specified) added L2 regularization NMF Support masking values add impute perturb methods crossValidate","code":""},{"path":"/news.html","id":"minor-changes-1","dir":"","previous_headings":"RcppML 0.5.1","what":"Minor changes","title":"NA","text":"better random initializations (now using rnorm runif multiple ranges/shapes) New vignette random restarts better “head” “show” methods return “w_init” model","code":""},{"path":[]},{"path":"/news.html","id":"major-changes-2","dir":"","previous_headings":"RcppML 0.5.2","what":"Major changes","title":"NA","text":"add linked NMF update documentation","code":""},{"path":"/news.html","id":"minor-changes-2","dir":"","previous_headings":"RcppML 0.5.2","what":"Minor changes","title":"NA","text":"clean C++ API C++ API gets meta-templating","code":""},{"path":[]},{"path":"/news.html","id":"major-changes-3","dir":"","previous_headings":"RcppML 1.0","what":"Major changes:","title":"NA","text":"better cross-validation, now exclusively using mean squared error missing value imputation (random speckled patterns missing values) complete migration S4 system, backwards compatibility CRAN version 0.5.0 new vignettes built-datasets","code":""},{"path":"/news.html","id":"minor-changes-3","dir":"","previous_headings":"RcppML 1.0","what":"Minor changes:","title":"NA","text":"compatibility latest version Matrix package","code":""},{"path":"/reference/align_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Align two matrices with bipartite matching — align_models","title":"Align two matrices with bipartite matching — align_models","text":"align S4 method `nmf` class, operates `w` matrices.","code":""},{"path":"/reference/align_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align two matrices with bipartite matching — align_models","text":"","code":"align_models(w, wref, method = \"cosine\", ...)"},{"path":"/reference/align_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align two matrices with bipartite matching — align_models","text":"w matrix columns aligned columns wref wref reference matrix columns w aligned ... additional arguments","code":""},{"path":"/reference/aml.html","id":null,"dir":"Reference","previous_headings":"","what":"Acute Myelogenous Leukemia cells — aml","title":"Acute Myelogenous Leukemia cells — aml","text":"DNA methylation ~800 regions 123 Acute Myelogenous Leukemia (AML) samples classified probable cell origin (GMP, L-MPP, MEP), 5 samples healthy references suspected cell origin (GMP, L-MPP, MEP).","code":""},{"path":"/reference/aml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Acute Myelogenous Leukemia cells — aml","text":"","code":"data(aml)"},{"path":"/reference/aml.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Acute Myelogenous Leukemia cells — aml","text":"list dense matrix features (methylated regions) rows samples (\"AML\" \"Control\" samples, classified putative cell origin reference cell type) columns. \"metadata_h\" maps publicly available clinical metadata.","code":""},{"path":"/reference/aml.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Acute Myelogenous Leukemia cells — aml","text":"AML methylation signatures differ cell origin additive methylation subtractive methylation, addition tumor-specific heterogeneity. AML tumors likely originated one three healthy cell types (GMP, LMPP, MEP), challenge classify cell origin based healthy cell type DMRs.","code":""},{"path":"/reference/bipartiteMatch.html","id":null,"dir":"Reference","previous_headings":"","what":"Bipartite graph matching — bipartiteMatch","title":"Bipartite graph matching — bipartiteMatch","text":"Hungarian algorithm matching samples bipartite graph distance (\"cost\") matrix","code":""},{"path":"/reference/bipartiteMatch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bipartite graph matching — bipartiteMatch","text":"","code":"bipartiteMatch(x)"},{"path":"/reference/bipartiteMatch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bipartite graph matching — bipartiteMatch","text":"x symmetric matrix giving cost every possible pairing","code":""},{"path":"/reference/bipartiteMatch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bipartite graph matching — bipartiteMatch","text":"List \"cost\" \"pairs\"","code":""},{"path":"/reference/bipartiteMatch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bipartite graph matching — bipartiteMatch","text":"implementation adapted RcppHungarian, Rcpp wrapper original C++ implementation Cong Ma (2016).","code":""},{"path":"/reference/bipartition.html","id":null,"dir":"Reference","previous_headings":"","what":"Bipartition a sample set — bipartition","title":"Bipartition a sample set — bipartition","text":"Spectral biparitioning rank-2 matrix factorization","code":""},{"path":"/reference/bipartition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bipartition a sample set — bipartition","text":"","code":"bipartition(data, tol = 1e-05, nonneg = TRUE, ...)"},{"path":"/reference/bipartition.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bipartition a sample set — bipartition","text":"data dense sparse matrix features rows samples columns. Prefer matrix Matrix::dgCMatrix, respectively tol tolerance fit nonneg enforce non-negativity rank-2 factorization used bipartitioning ... development parameters","code":""},{"path":"/reference/bipartition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bipartition a sample set — bipartition","text":"list giving bipartition useful statistics: v       : vector giving difference sample loadings factors rank-2 factorization dist    : relative cosine distance samples within cluster centroids assigned vs. -assigned cluster size1   : number samples first cluster (positive loadings 'v') size2   : number samples second cluster (negative loadings 'v') samples1: indices samples first cluster samples2: indices samples second cluster center1 : mean feature loadings across samples first cluster center2 : mean feature loadings across samples second cluster","code":""},{"path":"/reference/bipartition.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bipartition a sample set — bipartition","text":"Spectral bipartitioning popular subroutine divisive clustering. sign difference sample loadings factors rank-2 matrix factorization gives bipartition nearly identical SVD. Rank-2 matrix factorization alternating least squares faster rank-2-truncated SVD (.e. irlba). function specialization rank-2 nmf support factorization subset samples, additional calculations factorization model relevant bipartitioning. See nmf details regarding rank-2 factorization.","code":""},{"path":"/reference/bipartition.html","id":"advanced-parameters","dir":"Reference","previous_headings":"","what":"Advanced Parameters","title":"Bipartition a sample set — bipartition","text":"Several parameters may specified ... argument: diag = TRUE: scale factors \\(w\\) \\(h\\) sum 1 introducing diagonal, \\(d\\). generally never set FALSE. Diagonalization enables symmetry models factorization symmetric matrices, convex L1 regularization, consistent factor scalings. samples = 1:ncol(): samples include bipartition, numbered 1 ncol(). Default samples. calc_dist = TRUE: calculate relative cosine distance samples within cluster either cluster centroid. TRUE, centers clusters also calculated. seed = NULL: random seed model initialization, generally needed rank-2 factorizations robust solutions recovered diag = TRUE maxit = 100: maximum number alternating updates \\(w\\) \\(h\\). Generally, rank-2 factorizations converge quickly need adjusted.","code":""},{"path":"/reference/bipartition.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bipartition a sample set — bipartition","text":"Kuang, D, Park, H. (2013). \"Fast rank-2 nonnegative matrix factorization hierarchical document clustering.\" Proc. 19th ACM SIGKDD intl. conf. Knowledge discovery data mining.","code":""},{"path":[]},{"path":"/reference/bipartition.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Bipartition a sample set — bipartition","text":"Zach DeBruine","code":""},{"path":"/reference/bipartition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bipartition a sample set — bipartition","text":"","code":"if (FALSE) { library(Matrix) data(iris) A <- as(as.matrix(iris[,1:4]), \"dgCMatrix\") bipartition(A, calc_dist = TRUE) }"},{"path":"/reference/biplot-nmf-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Biplot for NMF factors — biplot,nmf-method","title":"Biplot for NMF factors — biplot,nmf-method","text":"Produces biplot output nmf","code":""},{"path":"/reference/biplot-nmf-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Biplot for NMF factors — biplot,nmf-method","text":"","code":"# S4 method for nmf biplot(x, factors = c(1, 2), matrix = \"w\", group_by = NULL, ...)"},{"path":"/reference/biplot-nmf-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Biplot for NMF factors — biplot,nmf-method","text":"x object class \"nmf\" factors length 2 vector specifying factors plot. matrix either w h group_by discrete factor giving groupings samples features. Must length number samples object$h number features object$w. ... consistency biplot generic","code":""},{"path":"/reference/biplot-nmf-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Biplot for NMF factors — biplot,nmf-method","text":"ggplot2 object","code":""},{"path":[]},{"path":"/reference/cosine.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine similarity — cosine","title":"Cosine similarity — cosine","text":"Column--column Euclidean norm cosine similarity matrix, pair matrices, pair vectors, pair vector matrix. Supports sparse matrices.","code":""},{"path":"/reference/cosine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine similarity — cosine","text":"","code":"cosine(x, y = NULL)"},{"path":"/reference/cosine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine similarity — cosine","text":"x matrix vector , coercible , class \"dgCMatrix\" \"sparseVector\" y (optional) matrix vector , coercible , class \"dgCMatrix\" \"sparseVector\"","code":""},{"path":"/reference/cosine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cosine similarity — cosine","text":"dense matrix, vector, value giving cosine distances","code":""},{"path":"/reference/cosine.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cosine similarity — cosine","text":"function takes advantage extremely fast vector operations able handle large datasets. cosine applies Euclidean norm provide similar results Pearson correlation. Note negative values may returned due use Euclidean normalization associations largely random. function adopts sparse matrix computational strategy applied qlcMatrix::cosSparse, extends combination single /pair sparse matrix /dense vector.","code":""},{"path":"/reference/crossValidate.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for NMF — crossValidate","title":"Cross-validation for NMF — crossValidate","text":"Find \"optimal\" rank Non-Negative Matrix Factorization using cross-validation. Returns data.frame class nmfCrossValidate. Plot results using plot class method.","code":""},{"path":"/reference/crossValidate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for NMF — crossValidate","text":"","code":"crossValidate(data, k, reps = 3, n = 0.05, verbose = FALSE, ...)  # S3 method for nmfCrossValidate plot(x, ...)"},{"path":"/reference/crossValidate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for NMF — crossValidate","text":"data dense sparse matrix features rows samples columns. Prefer matrix Matrix::dgCMatrix, respectively k array factorization ranks test reps number independent replicates run n fraction values handle missing (default 5%, 0.05) verbose updates displayed factorization completed ... parameters RcppML::nmf, including data k x nmfCrossValidate object, result crossValidate","code":""},{"path":"/reference/crossValidate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for NMF — crossValidate","text":"data.frame class nmfCrossValidate columns rep, k, value","code":""},{"path":"/reference/crossValidate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for NMF — crossValidate","text":"random speckled pattern values masked model fitting, mean squared error prediction evaluated model reached desired tolerance. rank model achieves lowest error (best prediction accuracy) optimal rank.","code":""},{"path":[]},{"path":"/reference/dclust.html","id":null,"dir":"Reference","previous_headings":"","what":"Divisive clustering — dclust","title":"Divisive clustering — dclust","text":"Recursive bipartitioning rank-2 matrix factorization efficient modularity-approximate stopping criteria","code":""},{"path":"/reference/dclust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Divisive clustering — dclust","text":"","code":"dclust(   A,   min_samples,   min_dist = 0,   tol = 1e-05,   maxit = 100,   nonneg = TRUE,   seed = NULL )"},{"path":"/reference/dclust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Divisive clustering — dclust","text":"matrix features--samples sparse format (preferred class \"Matrix::dgCMatrix\") min_samples stopping criteria giving minimum number samples permitted cluster min_dist stopping criteria giving minimum cosine distance samples within cluster center assigned vs. unassigned cluster. 0, neither distance cluster centroids calculated. tol rank-2 NMF, correlation distance (\\(1 - R^2\\)) \\(w\\) across consecutive iterations stop factorization maxit maximum number fitting iterations nonneg rank-2 NMF, enforce non-negativity seed random seed rank-2 NMF model initialization","code":""},{"path":"/reference/dclust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Divisive clustering — dclust","text":"list lists corresponding individual clusters: id      : character sequence \"0\" \"1\" giving position clusters along splitting hierarchy samples : indices samples cluster center  : mean feature expression samples cluster dist    : applicable, relative cosine distance samples cluster assigned/unassigned cluster center. leaf    : cluster leaf node","code":""},{"path":"/reference/dclust.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Divisive clustering — dclust","text":"Divisive clustering sensitive fast method sample classification. Samples recursively partitioned two groups stopping criteria satisfied prevents successful partitioning. See nmf bipartition technical considerations optimizations relevant bipartitioning. Stopping criteria. Two stopping criteria used prevent indefinite division clusters tune clustering resolution desirable range: min_samples: Minimum number samples permitted cluster min_dist: Minimum cosine distance samples cluster center relative unassigned cluster center (approximation Newman-Girvan modularity) Newman-Girvan modularity (\\(Q\\)) interpretable widely used measure modularity bipartition. However, requires calculation distance within-cluster -cluster sample pairs. computationally intensive, especially large sample sets. dclust uses measure linearly approximates Newman-Girvan modularity, simply requires calculation distance samples cluster cluster centers (assigned unassigned center), orders magnitude faster compute. Cosine distance used instead Euclidean distance since handles outliers sparsity well. bipartition rejected either two clusters contains fewer min_samples mean relative cosine distance bipartition less min_dist. bipartition attempted 2 * min_samples samples cluster, meaning dist may calculated clusters. Reproducibility. rank-2 NMF approximate requires random initialization, results may vary slightly across restarts. Therefore, specify seed guarantee absolute reproducibility. setting seed, reproducibility may improved setting tol smaller number increase exactness bipartition.","code":""},{"path":"/reference/dclust.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Divisive clustering — dclust","text":"Schwartz, G. et al. \"TooManyCells identifies visualizes relationships single-cell clades\". Nature Methods (2020). Newman, MEJ. \"Modularity community structure networks\". PNAS (2006) Kuang, D, Park, H. (2013). \"Fast rank-2 nonnegative matrix factorization hierarchical document clustering.\" Proc. 19th ACM SIGKDD intl. conf. Knowledge discovery data mining.","code":""},{"path":[]},{"path":"/reference/dclust.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Divisive clustering — dclust","text":"Zach DeBruine","code":""},{"path":"/reference/dclust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Divisive clustering — dclust","text":"","code":"if (FALSE) { data(USArrests) A <- Matrix::as(as.matrix(t(USArrests)), \"dgCMatrix\") clusters <- dclust(A, min_samples = 2, min_dist = 0.001) str(clusters) }"},{"path":"/reference/hawaiibirds.html","id":null,"dir":"Reference","previous_headings":"","what":"Bird species frequency in Hawaii — hawaiibirds","title":"Bird species frequency in Hawaii — hawaiibirds","text":"Frequency bird species observation within 1km-squared grids Hawaii recorded eBird project. counts complete grids sampled better others (.e. urban areas, birding hotspots).","code":""},{"path":"/reference/hawaiibirds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bird species frequency in Hawaii — hawaiibirds","text":"","code":"data(hawaiibirds)"},{"path":"/reference/hawaiibirds.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Bird species frequency in Hawaii — hawaiibirds","text":"list three components: counts giving mean total counts species grid, metadata_h giving information grid (.e. latitude, longitude, island), metadata_w giving information species taxonomic classification.","code":""},{"path":"/reference/hawaiibirds.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bird species frequency in Hawaii — hawaiibirds","text":"dataset obtained follows: eBird observations Hawaii downloaded eBird website Oct. 2021 complete checklists retained non-X counts retained Species removed fewer 50 observations spuhs Grids defined latitude/longitude rounded two decimal places Grids removed fewer 10 checklists Mean frequency species grid calculated based number times species observed number checklists submitted grid. Grids assigned one major Hawaiian island based geographical coordinates.","code":""},{"path":"/reference/lnmf.html","id":null,"dir":"Reference","previous_headings":"","what":"Linked non-negative matrix factorization — lnmf","title":"Linked non-negative matrix factorization — lnmf","text":"Run lNMF list datasets separate shared unique signals.","code":""},{"path":"/reference/lnmf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Linked non-negative matrix factorization — lnmf","text":"","code":"lnmf(   data,   k_wh,   k_uv,   tol = 1e-04,   maxit = 100,   L1 = c(0, 0),   L2 = c(0, 0),   nonneg = TRUE,   seed = NULL,   mask = NULL )"},{"path":"/reference/lnmf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Linked non-negative matrix factorization — lnmf","text":"data list dense sparse matrices giving features rows samples columns. Rows matrices must correspond exactly. Prefer matrix Matrix::dgCMatrix, respectively k_wh rank shared factorization k_uv ranks unique factorizations, array corresponding dataset provided tol tolerance fit maxit maximum number fitting iterations L1 LASSO penalties range (0, 1], single value array length two c(w & u, h & v) L2 Ridge penalties greater zero, single value array length two c(w & u, h & v) seed single initialization seed array seeds. multiple seeds provided, model least mean squared error returned. mask list dense sparse matrices indicating values data handle missing. Prefer Matrix::ngCMatrix. Alternatively, specify string \"zeros\" \"NA\" mask either zeros NA values.","code":""},{"path":"/reference/lnmf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Linked non-negative matrix factorization — lnmf","text":"object class lnmf","code":""},{"path":"/reference/lnmf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Linked non-negative matrix factorization — lnmf","text":"Detailed documentation come","code":""},{"path":"/reference/lnmf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Linked non-negative matrix factorization — lnmf","text":"DeBruine, ZJ, Melcher, K, Triche, TJ. (2021). \"High-performance non-negative matrix factorization large single-cell data.\" BioRXiv.","code":""},{"path":[]},{"path":"/reference/lnmf.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Linked non-negative matrix factorization — lnmf","text":"Zach DeBruine","code":""},{"path":"/reference/movielens.html","id":null,"dir":"Reference","previous_headings":"","what":"Movie ratings — movielens","title":"Movie ratings — movielens","text":"~250,000 Ratings ~3800 movies across 19 genres 610 users scale 1 5 stars. Zeros indicate rating.","code":""},{"path":"/reference/movielens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Movie ratings — movielens","text":"","code":"data(movielens)"},{"path":"/reference/movielens.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Movie ratings — movielens","text":"list two components: ratings matrix Matrix::dgCMatrix format movies (rows) vs. users (columns), genres sparse logical matrix Matrix::lgCMatrix format, giving genres (rows) movie (columns) assigned.","code":""},{"path":"/reference/movielens.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Movie ratings — movielens","text":"dataset derived https://grouplens.org/datasets/movielens/ (ml-latest-small.zip), movies without genre assignment removed. Half-star ratings rounded . Movies fewer 5 ratings removed.","code":""},{"path":"/reference/mse.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean squared error of factor model — mse","title":"Mean squared error of factor model — mse","text":"evaluate S4 method nmf class, allows one input `w`, `d`, `h`, `data` independently.","code":""},{"path":"/reference/mse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean squared error of factor model — mse","text":"","code":"mse(w, d = NULL, h, data, mask = NULL, missing_only = FALSE, ...)"},{"path":"/reference/mse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean squared error of factor model — mse","text":"w feature factor matrix (features rows) d scaling diagonal vector (applicable) h sample factor matrix (samples columns) ... additional arguments","code":""},{"path":"/reference/nmf-class-methods.html","id":null,"dir":"Reference","previous_headings":"","what":"nmf class methods — subset,nmf-method","title":"nmf class methods — subset,nmf-method","text":"Calculate mean squared error NMF model, accounting masking schemes requested fitting. Given NMF model form \\(= wdh\\), project projects w onto solve h.","code":""},{"path":"/reference/nmf-class-methods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"nmf class methods — subset,nmf-method","text":"","code":"# S4 method for nmf subset(x, i, ...)  # S4 method for nmf,ANY,ANY,ANY [(x, i)  # S4 method for nmf head(x, n = getOption(\"digits\"), ...)  # S4 method for nmf show(object)  # S4 method for nmf dimnames(x)  # S4 method for nmf dim(x)  # S4 method for nmf t(x)  # S4 method for nmf sort(x, decreasing = TRUE, ...)  # S4 method for nmf prod(x, ..., na.rm = FALSE)  # S4 method for nmf $(x, name)  # S4 method for nmf,list coerce(from, to)  # S4 method for nmf sparsity(object, ...)  # S4 method for nmf align(object, ref, method = \"cosine\", ...)  # S4 method for nmf evaluate(x, data, mask = NULL, missing_only = FALSE, ...)  # S4 method for nmf [[(x, i)  # S4 method for nmf predict(object, data, L1 = 0, L2 = 0, mask = NULL, ...)"},{"path":"/reference/nmf-class-methods.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"nmf class methods — subset,nmf-method","text":"x fitted model, class nmf, generally result calling nmf, models equal dimensions data indices ... arguments passed methods n number rows/columns show object fitted model, class nmf, generally result calling nmf, models equal dimensions data decreasing logical. sort increasing decreasing? na.rm remove na values name name nmf class slot class coerce method perform coercion class coerce method perform coercion ref reference nmf model object aligned method either cosine cor data dense sparse matrix features rows samples columns. Prefer matrix Matrix::dgCMatrix, respectively mask dense sparse matrix values data handle missing. Prefer Matrix::dgCMatrix. Alternatively, specify \"zeros\" \"NA\" mask either zeros NA values. missing_only calculate mean squared error missing values specified matrix mask L1 single LASSO penalty range (0, 1] L2 single Ridge penalty greater zero","code":""},{"path":"/reference/nmf-class-methods.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"nmf class methods — subset,nmf-method","text":"object class nmf","code":""},{"path":"/reference/nmf-class-methods.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"nmf class methods — subset,nmf-method","text":"nmf models, sparsity factor computed summarized \\(w\\) \\(h\\) matrices. long data.frame columns factor, sparsity, model returned. nmf models, factors object reordered minimize cost bipartite matching (see bipartiteMatch) cosine correlation distance matrix. \\(w\\) matrix used matching, must equidimensional object ref. : add documentation alternating least squares matrix factorization update problem \\(= wh\\), updates  (projection) \\(h\\) given equation: $$w^Twh = wA_j$$  form \\(ax = b\\) \\(= w^Tw\\) \\(x = h\\) \\(b = wA_j\\) columns \\(j\\) \\(\\). L1 penalty subtracted \\(b\\) generally scaled max(b), \\(b = WA_j\\) columns \\(j\\) \\(\\). easy way properly scale L1 penalty normalize columns \\(w\\) sum value (e.g. 1). scaling applied function. scaling guarantees L1 = 1 gives completely sparse solution. specializations dense sparse input matrices, symmetric input matrices, rank-1 rank-2 projections. See documentation nmf theoretical details guidance.","code":""},{"path":"/reference/nmf-class-methods.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"nmf class methods — subset,nmf-method","text":"DeBruine, ZJ, Melcher, K, Triche, TJ. (2021). \"High-performance non-negative matrix factorization large single-cell data.\" BioRXiv.","code":""},{"path":[]},{"path":"/reference/nmf-class-methods.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"nmf class methods — subset,nmf-method","text":"Zach DeBruine","code":""},{"path":"/reference/nmf-class-methods.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"nmf class methods — subset,nmf-method","text":"","code":"if (FALSE) { w <- matrix(runif(1000 * 10), 1000, 10) h_true <- matrix(runif(10 * 100), 10, 100) # A is the crossproduct of \"w\" and \"h\" with 10% signal dropout A <- (w %*% h_true) * (r_sparsematrix(1000, 100, 10) > 0) h <- project(w, A) cor(as.vector(h_true), as.vector(h))  # alternating projections refine solution (like NMF) mse_bad <- mse(w, rep(1, ncol(w)), h, A) # mse before alternating updates h <- project(w, A) w <- t(project(h, t(A))) h <- project(w, A) w <- t(project(h, t(A))) h <- project(w, A) w <- t(project(h, t(A))) mse_better <- mse(w, rep(1, ncol(w)), h, A) # mse after alternating updates mse_better < mse_bad }"},{"path":"/reference/nmf.html","id":null,"dir":"Reference","previous_headings":"","what":"Non-negative matrix factorization — nmf","title":"Non-negative matrix factorization — nmf","text":"High-performance NMF form \\(= wdh\\) large dense sparse matrices, returns object class nmf.","code":""},{"path":"/reference/nmf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Non-negative matrix factorization — nmf","text":"","code":"nmf(   data,   k,   tol = 1e-04,   maxit = 100,   L1 = c(0, 0),   L2 = c(0, 0),   seed = NULL,   mask = NULL,   ... )"},{"path":"/reference/nmf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Non-negative matrix factorization — nmf","text":"data dense sparse matrix features rows samples columns. Prefer matrix Matrix::dgCMatrix, respectively k rank tol tolerance fit maxit maximum number fitting iterations L1 LASSO penalties range (0, 1], single value array length two c(w, h) L2 Ridge penalties greater zero, single value array length two c(w, h) seed single initialization seed array, matrix list matrices giving initial w. multiple initializations, model least mean squared error returned. mask dense sparse matrix values data handle missing. Prefer Matrix::dgCMatrix. Alternatively, specify \"zeros\" \"NA\" mask either zeros NA values. ... development parameters","code":""},{"path":"/reference/nmf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Non-negative matrix factorization — nmf","text":"object class nmf","code":""},{"path":"/reference/nmf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Non-negative matrix factorization — nmf","text":"fast NMF implementation decomposes matrix \\(\\) lower-rank non-negative matrices \\(w\\) \\(h\\), columns \\(w\\) rows \\(h\\) scaled sum 1 via multiplication diagonal, \\(d\\): $$= wdh$$ scaling diagonal ensures convex L1 regularization, consistent factor scalings regardless random initialization, model symmetry factorizations symmetric matrices. factorization model randomly initialized.  \\(w\\) \\(h\\) updated alternating least squares. RcppML achieves high performance using Eigen C++ linear algebra library, OpenMP parallelization, dedicated Rcpp sparse matrix class, fast sequential coordinate descent non-negative least squares initialized Cholesky least squares solutions. Sparse optimization automatically applied input matrix sparse matrix (.e. Matrix::dgCMatrix). also specialized back-ends symmetric, rank-1, rank-2 factorizations. L1 penalization can used increasing sparsity factors assisting interpretability. Penalty values range 0 1, 1 gives complete sparsity. Set options(RcppML.verbose = TRUE) print model tolerances console iteration. Parallelization applied OpenMP using number threads getOption(\"RcppML.threads\") set option(RcppML.threads = 0), example. 0 corresponds threads, let OpenMP decide.","code":""},{"path":"/reference/nmf.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Non-negative matrix factorization — nmf","text":"w feature factor matrix d scaling diagonal vector h sample factor matrix misc list often containing components: tol     : tolerance fit iter    : number fitting updates runtime : runtime seconds mse     : mean squared error model (calculated multiple starts ) w_init  : initial w matrix used model fitting","code":""},{"path":"/reference/nmf.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Non-negative matrix factorization — nmf","text":"S4 methods available nmf class: predict: project NMF model (partial model) onto new samples evaluate: calculate mean squared error loss NMF model summary: data.frame giving fractional, total, mean representation factors samples features grouped criteria align: find ordering factors one nmf model best matches another nmf model prod: compute dense approximation input data sparsity: compute sparsity factor \\(w\\) \\(h\\) subset: subset, reorder, select, extract factors ([) generics dim, dimnames, t, show, head","code":""},{"path":"/reference/nmf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Non-negative matrix factorization — nmf","text":"DeBruine, ZJ, Melcher, K, Triche, TJ. (2021). \"High-performance non-negative matrix factorization large single-cell data.\" BioRXiv. Lin, X, Boutros, PC (2020). \"Optimization expansion non-negative matrix factorization.\" BMC Bioinformatics. Lee, D, Seung, HS (1999). \"Learning parts objects non-negative matrix factorization.\" Nature. Franc, VC, Hlavac, VC, Navara, M. (2005). \"Sequential Coordinate-Wise Algorithm Non-negative Least Squares Problem\". Proc. Int'l Conf. Computer Analysis Images Patterns. Lecture Notes Computer Science.","code":""},{"path":[]},{"path":"/reference/nmf.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Non-negative matrix factorization — nmf","text":"Zach DeBruine","code":""},{"path":"/reference/nmf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Non-negative matrix factorization — nmf","text":"","code":"if (FALSE) { # basic NMF model <- nmf(rsparsematrix(1000, 100, 0.1), k = 10)  # compare rank-2 NMF to second left vector in an SVD data(iris) A <- Matrix::as(as.matrix(iris[,1:4]), \"dgCMatrix\") nmf_model <- nmf(A, 2, tol = 1e-5) bipartitioning_vector <- apply(nmf_model$w, 1, diff) second_left_svd_vector <- base::svd(A, 2)$u[,2] abs(cor(bipartitioning_vector, second_left_svd_vector))  # compare rank-1 NMF with first singular vector in an SVD abs(cor(nmf(A, 1)$w[,1], base::svd(A, 2)$u[,1]))  # symmetric NMF A <- crossprod(rsparsematrix(100, 100, 0.02)) model <- nmf(A, 10, tol = 1e-5, maxit = 1000) plot(model$w, t(model$h)) # see package vignette for more examples }"},{"path":"/reference/nnls.html","id":null,"dir":"Reference","previous_headings":"","what":"Non-negative least squares — nnls","title":"Non-negative least squares — nnls","text":"Solves equation %*% x = b x subject \\(x > 0\\).","code":""},{"path":"/reference/nnls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Non-negative least squares — nnls","text":"","code":"nnls(   a,   b,   cd_maxit = 100L,   cd_tol = 1e-08,   fast_nnls = FALSE,   L1 = 0,   L2 = 0,   PE = 0 )"},{"path":"/reference/nnls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Non-negative least squares — nnls","text":"symmetric positive definite matrix giving coefficients linear system b matrix giving right-hand side(s) linear system cd_maxit maximum number coordinate descent iterations cd_tol stopping criteria, difference \\(x\\) across consecutive solutions sum \\(x\\) fast_nnls initialize coordinate descent FAST NNLS approximation L1 L1/LASSO penalty subtracted b L2 Ridge penalty added diagonal PE Pattern Extraction (angular) penalty added -diagonal values ","code":""},{"path":"/reference/nnls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Non-negative least squares — nnls","text":"vector matrix giving solution x","code":""},{"path":"/reference/nnls.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Non-negative least squares — nnls","text":"fast implementation non-negative least squares (NNLS), suitable small large systems. Algorithm. Sequential coordinate descent (CD) core implementation, requires initialization \\(x\\). two supported methods initialization \\(x\\): Zero-filled initialization fast_nnls = FALSE cd_maxit > 0. generally efficient well-conditioned small systems. Approximation FAST fast_nnls = TRUE. Forward active set tuning (FAST), described , finds approximate active set using unconstrained least squares solutions found Cholesky decomposition substitution. use FAST approximation, set cd_maxit = 0. must symmetric positive definite FAST NNLS used, checked. See BioRXiv manuscript (references) benchmarking Lawson-Hanson NNLS technical introduction methods. Coordinate Descent NNLS. Least squares sequential coordinate descent used ensure solution returned exact. algorithm introduced Franc et al. (2005), implementation vectorized optimized rendition found NNLM R package Xihui Lin (2020). FAST NNLS. Forward active set tuning (FAST) exact near-exact NNLS approximation initialized unconstrained least squares solution. Negative values unconstrained solution set zero (\"active set\"), values added  \"feasible set\". unconstrained least squares solution solved \"feasible set\", negative values resulting solution set zero, process repeated feasible set solution strictly positive. FAST algorithm definite convergence guarantee feasible set either converge become smaller iteration. result generally exact nearly exact small well-conditioned systems (< 50 variables) within 2 iterations thus sets coordinate descent rapid convergence. FAST method similar first phase -called \"TNT-NN\" algorithm (Myre et al., 2017), latter half method relies heavily heuristics refine approximate active set, avoid using coordinate descent instead.","code":""},{"path":"/reference/nnls.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Non-negative least squares — nnls","text":"DeBruine, ZJ, Melcher, K, Triche, TJ. (2021). \"High-performance non-negative matrix factorization large single-cell data.\" BioRXiv. Franc, VC, Hlavac, VC, Navara, M. (2005). \"Sequential Coordinate-Wise Algorithm Non-negative Least Squares Problem. Proc. Int'l Conf. Computer Analysis Images Patterns.\" Lin, X, Boutros, PC (2020). \"Optimization expansion non-negative matrix factorization.\" BMC Bioinformatics. Myre, JM, Frahm, E, Lilja DJ, Saar, MO. (2017) \"TNT-NN: Fast Active Set Method Solving Large Non-Negative Least Squares Problems\". Proc. Computer Science.","code":""},{"path":[]},{"path":"/reference/nnls.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Non-negative least squares — nnls","text":"Zach DeBruine","code":""},{"path":"/reference/nnls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Non-negative least squares — nnls","text":"","code":"if (FALSE) { # compare solution to base::solve for a random system X <- matrix(runif(100), 10, 10) a <- crossprod(X) b <- crossprod(X, runif(10)) unconstrained_soln <- solve(a, b) nonneg_soln <- nnls(a, b) unconstrained_err <- mean((a %*% unconstrained_soln - b)^2) nonnegative_err <- mean((a %*% nonneg_soln - b)^2) unconstrained_err nonnegative_err all.equal(solve(a, b), nnls(a, b))  # example adapted from multiway::fnnls example 1 X <- matrix(1:100,50,2) y <- matrix(101:150,50,1) beta <- solve(crossprod(X)) %*% crossprod(X, y) beta beta <- nnls(crossprod(X), crossprod(X, y)) beta }"},{"path":"/reference/project.html","id":null,"dir":"Reference","previous_headings":"","what":"Project a model onto new data — project","title":"Project a model onto new data — project","text":"Equivalent predict method NMF, requires w matrix supplied entire NMF model. Use NNLS project basis factor model onto new samples.","code":""},{"path":"/reference/project.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Project a model onto new data — project","text":"","code":"project(w, data, L1 = 0, L2 = 0, mask = NULL, ...)"},{"path":"/reference/project.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Project a model onto new data — project","text":"w matrix features (rows) factors (columns), corresponding rows data data dense sparse matrix L1 L1/LASSO penalty L2 L2/Ridge penalty mask masking data values","code":""},{"path":"/reference/project.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Project a model onto new data — project","text":"See nmf info, well predict method NMF.","code":""},{"path":"/reference/random.html","id":null,"dir":"Reference","previous_headings":"","what":"Random distributions and samples — r_sample","title":"Random distributions and samples — r_sample","text":"r_sample just like base::sample, faster. r_sample takes sample specified size elements x using replacement indicated. functions generate random distributions (uniform, normal, binomial) just like base R counterparts (runif, rnorm, rbinom), faster.","code":""},{"path":"/reference/random.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random distributions and samples — r_sample","text":"","code":"r_sample(x, size = NULL, replace = FALSE)  r_unif(n, min = 0, max = 1)  r_binom(n, size = 1, inv_prob = 2)"},{"path":"/reference/random.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random distributions and samples — r_sample","text":"x either positive integer giving number items choose , vector elements shuffle choose. See 'Details'. size number trials (one ) replace sampling replacement? n number observations min finite lower limit uniform distribution max finite upper limit uniform distribution inv_prob inverse probability success trial, must integral (e.g. 50 percent success = 2, 10 percent success = 10)","code":""},{"path":"/reference/random.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Random distributions and samples — r_sample","text":"RNGs make use Marsaglia's xorshift method generate random integers. r_unif takes random integer divides seed returns floating decimal portion result.","code":""},{"path":[]},{"path":"/reference/random.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random distributions and samples — r_sample","text":"","code":"# draw all integers from 1 to 10 in a random order sample(10) #>  [1]  2 10  3  7  9  1  4  8  6  5  # shuffle a vector of values v <- r_unif(3) v #> [1] 0.5913238 0.9289894 0.4358602 v_ <- sample(v) v_ #> [1] 0.5913238 0.9289894 0.4358602  # draw values from a vector sample(r_unif(100), 3) #> [1] 0.55872428 0.24705647 0.03172755  # draw some integers between 1 and 1000 sample(1000, 3) #> [1] 922  73 606  # simulate a uniform distribution v <- r_unif(10000) plot(density(v))   # simulate a binomial distribution v <- r_binom(10000, 100, inv_prob = 10) hist(v)  sum(v) / length(v) #> [1] 10.04 # ~10 because 100 trials at 10 percent success odds #   is about 10 successes per element  # get successful trials in a bernoulli distribution v <- r_binom(100, 1, 20) successful_trials <- slot(as(v, \"nsparseVector\"), \"i\") successful_trials #> [1] 14 36 59 60 68 98"},{"path":"/reference/RcppML.html","id":null,"dir":"Reference","previous_headings":"","what":"RcppML: Rcpp Machine Learning Library — RcppML","title":"RcppML: Rcpp Machine Learning Library — RcppML","text":"High-performance non-negative matrix factorization linear model projection sparse matrices, fast non-negative least squares implementations","code":""},{"path":"/reference/RcppML.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"RcppML: Rcpp Machine Learning Library — RcppML","text":"Zach DeBruine","code":""},{"path":"/reference/r_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Random transpose-identical dense/sparse matrix — r_matrix","title":"Random transpose-identical dense/sparse matrix — r_matrix","text":"Generate random sparse matrix, just like Matrix::rsparsematrix (matrix(runif(nrow * ncol), nrow,)), much faster. Generation transpose-identical matrices also supported without additional computational cost.","code":""},{"path":"/reference/r_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random transpose-identical dense/sparse matrix — r_matrix","text":"","code":"r_matrix(nrow, ncol, transpose_identical = FALSE)  r_sparsematrix(   nrow,   ncol,   inv_density,   transpose_identical = FALSE,   pattern = FALSE )"},{"path":"/reference/r_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random transpose-identical dense/sparse matrix — r_matrix","text":"nrow number rows ncol number columns transpose_identical matrix transpose-identical? inv_density integer giving inverse density matrix (.e. 10 percent density corresponds inv_density = 10). Density probabilistic, exact. See examples. pattern pattern matrix (Matrix::ngCMatrix) returned? , Matrix::dgCMatrix random uniform values returned.","code":""},{"path":[]},{"path":"/reference/r_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random transpose-identical dense/sparse matrix — r_matrix","text":"","code":"# generate a simple random matrix A <- r_matrix(10, 10)  # generate two matrices that are transpose identical set.seed(123) A1 <- r_matrix(10, 100, transpose_identical = TRUE) set.seed(123) A2 <- r_matrix(100, 10, transpose_identical = TRUE) all.equal(t(A2), A1) #> [1] TRUE  # generate a transpose-identical pair of speckled matrices set.seed(123) A <- r_sparsematrix(10, 100, inv_density = 10, transpose_identical = TRUE) set.seed(123) A <- r_sparsematrix(100, 10, inv_density = 10, transpose_identical = TRUE) all.equal(t(A), A) #> [1] \"Attributes: < Component \\\"Dim\\\": Mean relative difference: 1.636364 >\" #> [2] \"Attributes: < Component \\\"i\\\": Mean relative difference: 9.509434 >\"   #> [3] \"Attributes: < Component \\\"p\\\": Numeric: lengths (101, 11) differ >\"    #> [4] \"Attributes: < Component \\\"x\\\": Mean relative difference: 0.612613 >\"   isSymmetric(A[1:10, 1:10]) #> Error in UseMethod(\"isSymmetric\"): no applicable method for 'isSymmetric' applied to an object of class \"c('dgCMatrix', 'CsparseMatrix', 'dsparseMatrix', 'generalMatrix', 'dCsparseMatrix', 'dMatrix', 'sparseMatrix', 'compMatrix', 'Matrix', 'xMatrix', 'mMatrix', 'replValueSp')\" heatmap(as.matrix(A), scale = \"none\", Rowv = NA, Colv = NA)   # note that density is probabilistic, not absolute A <- replicate(1000, r_sparsematrix(100, 100, 10)) densities <- sapply(A, function(x) length(x@i) / prod(dim(x))) plot(density(densities)) # normal distribution centered at 0.100  range(densities) #> [1] 0.0903 0.1103"},{"path":"/reference/simulateNMF.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate an NMF dataset — simulateNMF","title":"Simulate an NMF dataset — simulateNMF","text":"Generate random matrix follows defined NMF model test NMF factorizations. Adapts methods NMF::syntheticNMF.","code":""},{"path":"/reference/simulateNMF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate an NMF dataset — simulateNMF","text":"","code":"simulateNMF(nrow, ncol, k, noise = 0.5, dropout = 0.5, seed = NULL)"},{"path":"/reference/simulateNMF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate an NMF dataset — simulateNMF","text":"nrow number rows ncol number columns k true rank simulated model noise standard deviation Gaussian noise centered 0 add input matrix. negative values noise addition set 0. dropout density dropout events seed seed random number generation","code":""},{"path":"/reference/simulateNMF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate an NMF dataset — simulateNMF","text":"list dense matrix true w h models","code":""},{"path":"/reference/summary-nmf-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize NMF factors — summary,nmf-method","title":"Summarize NMF factors — summary,nmf-method","text":"summary method class \"nmf\". Describes metadata representation NMF factors. Returns object class nmfSummary. Plot results using plot.","code":""},{"path":"/reference/summary-nmf-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize NMF factors — summary,nmf-method","text":"","code":"# S4 method for nmf summary(object, group_by, stat = \"sum\", ...)  # S3 method for nmfSummary plot(x, ...)"},{"path":"/reference/summary-nmf-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize NMF factors — summary,nmf-method","text":"object object class \"nmf\", usually, result call nmf group_by discrete factor giving groupings samples features. Must length number samples object$h number features object$w. stat either sum (sum factor weights falling within group), mean (mean factor weight falling within group). ... arguments passed methods x nmfSummary object, result calling summary nmf object","code":""},{"path":"/reference/summary-nmf-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize NMF factors — summary,nmf-method","text":"data.frame columns group, factor, stat","code":""},{"path":[]},{"path":"/news/index.html","id":"major-changes-0-5-0","dir":"Changelog","previous_headings":"","what":"Major changes","title":"RcppML 0.5.0","text":"Launch pkgdown site Added nmf S3 class result nmf function Introduce S3 methods NMF ([, align, biplot, dim, dimnames, head, mse, predict, print, prod, sort, sparsity, summary, t) New plotting methods NMF (biplot.nmf, plot.nmfSummary, plot.nmfCrossValidation) mse now S3 method nmf objects project now handles projections w, simplicity New vignette Getting Started NMF!","code":""},{"path":"/news/index.html","id":"minor-changes-0-5-0","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"RcppML 0.5.0","text":"Support specific sample feature selections NMF removed increase performance C++ end Removed updateInPlace advanced parameter nmf advantages convincing mask_zeros implementation now specific sparse matrices, multi-thread parallelization, projections transposition Added cosine function fast cosine distance calculations Condensed pared documentation throughout. Advanced usage discussion moved future vignettes.","code":""},{"path":[]},{"path":"/news/index.html","id":"major-changes-0-5-1","dir":"Changelog","previous_headings":"","what":"Major changes","title":"RcppML 0.5.1","text":"three new datasets (hawaiibirds, aml, movielens) Move NMF models methods S3 S4 stability Better random initializations (now using rnorm runif multiple ranges/shapes, multiple seeds specified) added L2 regularization NMF Support masking values add impute perturb methods crossValidate","code":""},{"path":"/news/index.html","id":"minor-changes-0-5-1","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"RcppML 0.5.1","text":"better random initializations (now using rnorm runif multiple ranges/shapes) New vignette random restarts better “head” “show” methods return “w_init” model","code":""},{"path":[]},{"path":"/news/index.html","id":"major-changes-0-5-2","dir":"Changelog","previous_headings":"","what":"Major changes","title":"RcppML 0.5.2","text":"add linked NMF update documentation","code":""},{"path":"/news/index.html","id":"minor-changes-0-5-2","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"RcppML 0.5.2","text":"clean C++ API C++ API gets meta-templating","code":""},{"path":[]},{"path":"/news/index.html","id":"major-changes-1-0","dir":"Changelog","previous_headings":"","what":"Major changes:","title":"RcppML 1.0","text":"better cross-validation, now exclusively using mean squared error missing value imputation (random speckled patterns missing values) complete migration S4 system, backwards compatibility CRAN version 0.5.0 new vignettes built-datasets","code":""},{"path":"/news/index.html","id":"minor-changes-1-0","dir":"Changelog","previous_headings":"","what":"Minor changes:","title":"RcppML 1.0","text":"compatibility latest version Matrix package","code":""}]
