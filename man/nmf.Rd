% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nmf.R, R/nmf_methods.R
\docType{class}
\name{nmf}
\alias{nmf}
\alias{nmf,}
\alias{nmf-class}
\title{Non-negative matrix factorization}
\usage{
nmf(
  data,
  k,
  tol = 1e-04,
  maxit = 100,
  L1 = c(0, 0),
  L2 = c(0, 0),
  seed = NULL,
  mask = NULL,
  ...
)
}
\arguments{
\item{data}{dense or sparse matrix of features in rows and samples in columns. Prefer \code{matrix} or \code{Matrix::dgCMatrix}, respectively}

\item{k}{rank}

\item{tol}{tolerance of the fit}

\item{maxit}{maximum number of fitting iterations}

\item{L1}{LASSO penalties in the range (0, 1], single value or array of length two for \code{c(w, h)}}

\item{L2}{Ridge penalties greater than zero, single value or array of length two for \code{c(w, h)}}

\item{seed}{single initialization seed or array, or a matrix or list of matrices giving initial \code{w}. For multiple initializations, the model with least mean squared error is returned.}

\item{mask}{dense or sparse matrix of values in \code{data} to handle as missing. Prefer \code{Matrix::dgCMatrix}. Alternatively, specify "\code{zeros}" or "\code{NA}" to mask either all zeros or NA values.}

\item{...}{development parameters}
}
\value{
object of class \code{nmf}
}
\description{
High-performance NMF of the form \eqn{A = wdh} for large dense or sparse matrices, returns an object of class \code{nmf}.
}
\details{
This fast NMF implementation decomposes a matrix \eqn{A} into lower-rank non-negative matrices \eqn{w} and \eqn{h},
with columns of \eqn{w} and rows of \eqn{h} scaled to sum to 1 via multiplication by a diagonal, \eqn{d}: \deqn{A = wdh}

The scaling diagonal ensures convex L1 regularization, consistent factor scalings regardless of random initialization, and model symmetry in factorizations of symmetric matrices.

The factorization model is randomly initialized.  \eqn{w} and \eqn{h} are updated by alternating least squares.

RcppML achieves high performance using the Eigen C++ linear algebra library, OpenMP parallelization, a dedicated Rcpp sparse matrix class, and fast sequential coordinate descent non-negative least squares initialized by Cholesky least squares solutions.

Sparse optimization is automatically applied if the input matrix \code{A} is a sparse matrix (i.e. \code{Matrix::dgCMatrix}). There are also specialized back-ends for symmetric, rank-1, and rank-2 factorizations.

L1 penalization can be used for increasing the sparsity of factors and assisting interpretability. Penalty values should range from 0 to 1, where 1 gives complete sparsity.

Set \code{options(RcppML.verbose = TRUE)} to print model tolerances to the console after each iteration.

Parallelization is applied with OpenMP using the number of threads in \code{getOption("RcppML.threads")} and set by \code{option(RcppML.threads = 0)}, for example. \code{0} corresponds to all threads, let OpenMP decide.
}
\section{Slots}{

\describe{
\item{\code{w}}{feature factor matrix}

\item{\code{d}}{scaling diagonal vector}

\item{\code{h}}{sample factor matrix}

\item{\code{misc}}{list often containing components:
\itemize{
  \item tol     : tolerance of fit
  \item iter    : number of fitting updates
  \item runtime : runtime in seconds
  \item mse     : mean squared error of model (calculated for multiple starts only)
  \item w_init  : initial w matrix used for model fitting
}}
}}

\section{Methods}{

S4 methods available for the \code{nmf} class:
\itemize{
\item \code{predict}: project an NMF model (or partial model) onto new samples
\item \code{evaluate}: calculate mean squared error loss of an NMF model
\item \code{summary}: \code{data.frame} giving \code{fractional}, \code{total}, or \code{mean} representation of factors in samples or features grouped by some criteria
\item \code{align}: find an ordering of factors in one \code{nmf} model that best matches those in another \code{nmf} model
\item \code{prod}: compute the dense approximation of input data
\item \code{sparsity}: compute the sparsity of each factor in \eqn{w} and \eqn{h}
\item \code{subset}: subset, reorder, select, or extract factors (same as \code{[})
\item generics such as \code{dim}, \code{dimnames}, \code{t}, \code{show}, \code{head}
}
}

\examples{
\dontrun{
# basic NMF
model <- nmf(rsparsematrix(1000, 100, 0.1), k = 10)

# compare rank-2 NMF to second left vector in an SVD
data(iris)
A <- Matrix::as(as.matrix(iris[, 1:4]), "dgCMatrix")
nmf_model <- nmf(A, 2, tol = 1e-5)
bipartitioning_vector <- apply(nmf_model$w, 1, diff)
second_left_svd_vector <- base::svd(A, 2)$u[, 2]
abs(cor(bipartitioning_vector, second_left_svd_vector))

# compare rank-1 NMF with first singular vector in an SVD
abs(cor(nmf(A, 1)$w[, 1], base::svd(A, 2)$u[, 1]))

# symmetric NMF
A <- crossprod(rsparsematrix(100, 100, 0.02))
model <- nmf(A, 10, tol = 1e-5, maxit = 1000)
plot(model$w, t(model$h))
# see package vignette for more examples
}
}
\references{
DeBruine, ZJ, Melcher, K, and Triche, TJ. (2021). "High-performance non-negative matrix factorization for large single-cell data." BioRXiv.

Lin, X, and Boutros, PC (2020). "Optimization and expansion of non-negative matrix factorization." BMC Bioinformatics.

Lee, D, and Seung, HS (1999). "Learning the parts of objects by non-negative matrix factorization." Nature.

Franc, VC, Hlavac, VC, Navara, M. (2005). "Sequential Coordinate-Wise Algorithm for the Non-negative Least Squares Problem". Proc. Int'l Conf. Computer Analysis of Images and Patterns. Lecture Notes in Computer Science.
}
\seealso{
\code{\link{project}}, \code{\link{mse}}, \code{\link{nnls}}
}
\author{
Zach DeBruine
}
